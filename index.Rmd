---
title: "Hit Prediction"
author: "Kat Wilson"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Intro to the Spotify API - Keys and Popularity Index

Using the Spotify API, I pulled the discogrpahies of 15 bands that cross range of musical style. With over 100 songs per band, the Spotify API gives us data on 40 measured variables, including album name, track name, duration, tempo, and many more.


```{r fig.height=5, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}
library(tidyverse)
load("/cloud/project/data/animal_chapter_data.Rdata")
load("/cloud/project/data/output_bayes (2).Rdata")
merge <- new_data
colnames(merge)
table(merge$artist_name)

```

Looking at the count of raw data, the distribution of keys looks similar to what others have determined to be the most frequented keys on Spotify. The data sample's distribution reflects the distribution of all keys on Spotify: keys of G major, D major and C major (more simple keys with less sharps and flats) are represented the most. 

Key distribution doesn't hold across artists. The band Radiohead wrote more songs in the key of A sharp than Animal Collective, who wrote more in D. This begs the question of how hits are formed among artists. Does a band get better by writing na lot in one key? Could other bands be more likely to land a hit when they write outside of their frequented key?

```{r fig.height=5, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}
par(mfrow=c(1,1))
clean_data_count <- new_data %>%
  group_by(key_mode) %>%
  mutate(value = n()) %>%
  select(key_mode, mode_name, value) %>%
  na.omit()
clean_data_count$mode_name <- as.factor(clean_data_count$mode_name)
clean_data_count$key_mode <- as.factor(clean_data_count$key_mode)
ggplot(clean_data_count, aes(x = key_mode,
                             fill = mode_name, y = value)) +
  geom_bar(stat = "identity",position = "dodge") +
  theme(axis.text.x = element_text(angle = 40),
        legend.title = element_blank(),
        panel.background = element_blank())+
  ylab("Count") +
  xlab("") +
  scale_fill_manual("legend", values = c("major" = "salmon",
                                         "minor" = "grey"))



```

Spotify's popularity index is a number from 0-100 that represents the total number of plays a track has. The popularity of songs in each key within each band will vary based on the variability of that band's key usage.

To determine this,, we only need the artist name, track name, key and popularity index. Let's also remove Live tracks, as the popularity of a live version might conflate the original studio recorded version. A summary statistic (the mean popularity) for each artists' key is computed, from these tracks, and converted to a decimal (probability).

```{r fig.height=5, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}
merge <- merge %>%
  select(track_name, artist_name, key_mode, popularity_index)
merge <- merge %>%
  arrange(track_name, popularity_index) %>%
  filter(duplicated(track_name) == FALSE)
merge <- merge[- grep("-", merge$track_name),]
merge <- merge[- grep("live", merge$track_name),]
merge <- merge[- grep("Live", merge$track_name),]
merged_new<- merge %>%
  group_by(artist_name, key_mode) %>%
  select(artist_name, key_mode, popularity_index, track_name) %>%
  filter(popularity_index >0)
merged_new$popularity_index <- as.numeric(merged_new$popularity_index)
merged_new <- merged_new %>%
  group_by(artist_name, key_mode) %>%  
  mutate(n = n()) %>%
  ungroup() %>%
  group_by(artist_name,key_mode)%>%
  summarise(mean_pop = mean(popularity_index)) %>%
  mutate(mean_pop = mean_pop/100)
head(merged_new)
```
We can visualize popularity across keys for this group of artists. Are these differences due to listener demand or due to artist intend? Maybe Nirvana fans just more "down". In that case, the Eminor key will be more popular than the E minor key among Beyonce fans? Nirvana also wrote more songs in the keys of C and G major than any other key. However, their most popular songs are in more experimental keys, such as Dsharp minor, F minor, and E minor. This begs the question of what we come to expect from a band, and what turns out to be popular among fans. The Velvet Underground only has one song in F minor, but the song (Sunday Morning, off of Velvet Underground and Nico) is their most popular. The Velvet Underground wrote the most songs in the key of D major, and there is not much variability in this key. Could it be because they are less inclined to take risks in that key? The final section will touch on this concept of hierarchical variance. First, though, let’s use Bayesian prediction to predict hit tracks.

```{r fig.height=5, fig.width=7, message=FALSE, warning=FALSE, include=FALSE}
### aggregated data
aggregated_data <- merged_new
save(aggregated_data, file = "/cloud/project/data/aggregated_data.Rdata")
## only Velvet Underground
load("/cloud/project/data/total_data.Rdata")
merged_new<- merged_new %>%
  filter(artist_name == "The Velvet Underground") %>%
  group_by(artist_name, key_mode) %>%
  select(artist_name, key_mode, popularity_index) %>%
  filter(popularity_index >0)
merged_new$popularity_index <- as.numeric(merged_new$popularity_index)
merged_new <- merged_new %>%
  group_by(artist_name, key_mode) %>%  
  mutate(n = n()) %>%
  ungroup() %>%
  group_by(artist_name,key_mode, n) %>%
  summarise(mean_pop = popularity_index) %>%
  mutate(mean_pop = mean_pop/100)
velvet_underground<- merged_new
save(velvet_underground, file = "/cloud/project/data/velvet_underground.Rdata")


## only Beyonce
load("/cloud/project/data/total_data.Rdata")
merged_new<- merged_new %>%
  filter(artist_name == "Beyoncé") %>%
  group_by(artist_name, key_mode) %>%
  select(artist_name, key_mode, popularity_index) %>%
  filter(popularity_index >0)
merged_new$popularity_index <- as.numeric(merged_new$popularity_index)
merged_new <- merged_new %>%
  group_by(artist_name, key_mode) %>%  
  mutate(n = n()) %>%
  ungroup() %>%
  group_by(artist_name,key_mode, n) %>%
  summarise(mean_pop = popularity_index) %>%
  mutate(mean_pop = mean_pop/100)
beyonce<- merged_new
save(beyonce, file = "/cloud/project/data/beyonce.Rdata")

## only Cash
load("/cloud/project/data/total_data.Rdata")
merged_new<- merged_new %>%
  filter(artist_name == "Johnny Cash") %>%
  group_by(artist_name, key_mode) %>%
  select(artist_name, key_mode, popularity_index) %>%
  filter(popularity_index >0)
merged_new$popularity_index <- as.numeric(merged_new$popularity_index)
merged_new <- merged_new %>%
  group_by(artist_name, key_mode) %>%  
  mutate(n = n()) %>%
  ungroup() %>%
  group_by(artist_name,key_mode, n) %>%
  summarise(mean_pop = popularity_index) %>%
  mutate(mean_pop = mean_pop/100)
cash <- merged_new
save(cash, file = "/cloud/project/data/cash.Rdata")


load("/cloud/project/data/aggregated_data.Rdata")
set <- as.data.frame(table(merged_new$artist_name, merged_new$key_mode))
load("/cloud/project/data/velvet_underground.Rdata")
load("/cloud/project/data/beyonce.Rdata")
load("/cloud/project/data/cash.Rdata")
clean_data <- merged_new

par(mfrow=c(2,2))
####
### step 2: visualize popularity
####
library(ggpubr)
library(gridExtra)
clean_data <-
  clean_data %>%
  mutate(key_mode = fct_reorder(key_mode, mean_pop))
## only Vevlet
velvet_underground <- velvet_underground %>%
  filter(artist_name == "The Velvet Underground")
## only Beyoncé
Beyoncé <- beyonce %>%
  filter(artist_name == "Beyoncé")
## only Cash
cash <- cash %>%
  filter(artist_name == "Johnny Cash")

```

```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}
'All Data' <- aggregated_data
'Velvet Underground' <- velvet_underground
'Beyoncé' <- beyonce
'Johnny Cash' <- cash
df_list <- list(`All Data`, `Velvet Underground`, `Beyoncé`, `Johnny Cash`)
for (i in 1:length(df_list)) {
  data <- as.data.frame(df_list[i])
b<- ggplot(data = data, aes(x= reorder(factor(key_mode), mean_pop), y = mean_pop)) +
  stat_summary(fun.data = mean_se) +
  coord_flip() +
  ggtitle(as.character(data[1,1]))+
  ylab("Mean Popularity Score") +
  xlab(NULL) +
  theme_minimal()
assign(paste("plot", as.character(data[1,1])), b)
}
ggarrange(ggarrange(`plot Allman Brothers Band`, `plot Beyoncé`,
                    `plot Johnny Cash`, `plot The Velvet Underground`))


```


## Bayesian Modeling Approach

I fit a two-component mixture model to the distribution of popularity averages, one that represents songs with average popularity, and another that represent hit tracks. We don't observe the grouping of songs in the data generation process (i.e. Spotify doesn't have a 'hit track' variable); therefore, this can be considered a missing data problem. The EM algorithm will help to identify which of the two components the data came from. A normal-normal mixture model will do well here, as the distribution of popularity appears to be bimodal. 

```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}
load("/cloud/project/data/aggregated_data.Rdata")
clean_data <- aggregated_data
#Calculating mean_pop
clean_data$mean_pop <- as.numeric(clean_data$mean_pop)
mean_pop <- clean_data$mean_pop
hist(mean_pop)
artist <-  clean_data$artist_name
key_mode <- clean_data$key_mode
```



$$\begin{equation}
y =  Normal(\mu, \sigma^2)
\end{equation}$$

$$\begin{equation}
y =  Normal(\mu, \sigma^2)
\end{equation}$$

We have 5 variance components: the mu (means) of the first two groups, the sigma squares (variances) of the two groups, and the alpha, representing the probability that any particular data point (song) is from the 1 component. A point from the 0 component is represented by 1-alpha. The five unknown parameters are given by

$$\begin{equation}
\theta = (\mu_1, \mu_0, \sigma_1^2, \sigma_0^2, \alpha)
\end{equation}$$

Estimating theta is difficult because we don't know which of the two components each observation came from. If we had the missing data, we could seperate these units out into the two components, but since we don't observe the data generation process, let's consider the missing data as an indicator. Each unit (song) has an indicator, equal to 1 if the observation came from group 1, and equal to 0 if the observation came from group 0. Alpha is the proportion of observations that are in group 1 and 0. In a counter-intuitive concept, adding more "carefully chosen parameters" (i.e. these indicators) to the model helps improve estimation.

The likelihood for the EM is written as a function of both the observed data and the missing data, where Y is the observed data, I is missing data. Observed and missing data are both conditioned on theta, as opposed to only observed variables conditioned on theta, which is the observed data likelihood.

$$\begin{equation}
P(Y, I | \theta)
\end{equation}$$

Again, this likelihood is not actually observable, because we don't have the value of indicators. Since we can't use the actual values (i), we plug in their expected values (yhat) to the actual likelihood. The expected value of an indicator is the probability of that observation (song) being 1. 

In the first part of EM, the expectation step, the expected indicator for every single observation (whether the observation is in the popular or un-popular group) is calculated. The following step (M step) takes these indicator values from the E step, and outputs new values for the parameters. In other words, the E step takes in current parameter values and outputs estimated values for the indicators. The M step takes in values for the indicators and outputs values for the parameters. The point of the M step is to give better values for the E step, where every step improves the estimates of the unknown. As a hill climbing algorithm, EM is guaranteed to find a local mode, not a global mode. 

**Expectation Step**

E step takes in the current parameter values:

```{r fig.height=4, fig.width=4, message=FALSE, warning=FALSE, include=TRUE}
#Expectation function
Estep <- function(y,alpha,mu0,mu1,sigsq0,sigsq1){
  n <- length(y)  
  ind <- rep(NA,n)
  for (i in 1:n){
    prob0 <- (1-alpha)*dnorm(y[i],mean=mu0,sd=sqrt(sigsq0))
    prob1 <- alpha*dnorm(y[i],mean=mu1,sd=sqrt(sigsq1))
    ind[i] <- prob1/(prob0+prob1)
  }
  ind
}
```

**Maximization Step**
M step otputs new values for the parametres, and estimates the parameters given the indicators:

```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}
#Maximization function
Mstep <- function(y,ind){
  n <- length(y)
  alpha <- sum(ind)/n
  mu1 <- sum(ind*y)/sum(ind)
  mu0 <- sum((1-ind)*y)/sum(1-ind)
  sigsq1 <- sum(ind*((y-mu1)^2))/sum(ind)
  sigsq0 <- sum((1-ind)*((y-mu0)^2))/sum(1-ind)
  c(alpha,mu0,mu1,sigsq0,sigsq1)
}
```

Choose an arbitrary starting point (curalpha as 0.5), and run the while loop for 100 iterations.
```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}
##observed data log likelihood function
loglik.mix <- function(y,ind,alpha,mu0,mu1,sigsq0,sigsq1){
  loglik <- sum(log(alpha*dnorm(y,mu1,sqrt(sigsq1))+(1-alpha)*dnorm(y,mu0,sqrt(sigsq0))))
  loglik
}
```


Estimates result in the overall maximum log-likelihood (=360.7) with mu0 at 0.17, mu1 at .33., and alpha at 0.52. This model assigns a high probability to a song being a hit. Notice the overlap in distributions.


```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}

#Running EM iterations
curalpha <- 0.5
curmu0 <- 0.1
curmu1 <- 0.4
cursigsq0 <- 0.03
cursigsq1 <- 0.03
mean(mean_pop)
curind <- Estep(mean_pop,curalpha,curmu0,curmu1,cursigsq0,cursigsq1)
loglik <- loglik.mix(mean_pop,curind,curalpha,curmu0,curmu1,cursigsq0,cursigsq1)
itermat <- c(curalpha,curmu0,curmu1,cursigsq0,cursigsq1,loglik)
diff <- 1
numiters <- 1
while (diff > 0.001 || numiters <= 100){
  curind <- Estep(mean_pop,curalpha,curmu0,curmu1,cursigsq0,cursigsq1)
  curparam <- Mstep(mean_pop,curind)
  curalpha <- curparam[1]
  curmu0 <- curparam[2]
  curmu1 <- curparam[3]
  cursigsq0 <- curparam[4]
  cursigsq1 <- curparam[5]
  itermat <- rbind(itermat,c(curparam,loglik))
  loglik <- loglik.mix(mean_pop,curind,curalpha,curmu0,curmu1,cursigsq0,cursigsq1)
  numiters <- numiters + 1
  diff <- max(abs(itermat[numiters,]-itermat[numiters-1,])) 
  # print (c(numiters,loglik))
}
parametertext <- c("alpha","mu0","mu1","sigsq0","sigsq1","loglik")
par(mfrow=c(2,3))
for (i in 1:6){
  plot(1:numiters,itermat[,i],main=parametertext[i],xlab="Iterations",ylab="Value")
}
# plotting fitted mixture density
finalparam<-itermat[numiters,]
alpha <- finalparam[1]
mu0 <- finalparam[2]
mu1 <- finalparam[3]
sigsq0 <- finalparam[4]
sigsq1 <- finalparam[5]
par(mfrow=c(1,1))

plot1<- hist(mean_pop,prob=T, main= "MLE Densities for Unique Variances")
x <- ppoints(1000)*0.9
y1 <- (1-alpha)*dnorm(x,mu0,sqrt(sigsq0))
y2 <- alpha*dnorm(x,mu1,sqrt(sigsq1))
## mu 0 is 0.07, and mu1 is .32
lines(x,y1,col=2)
lines(x,y2,col=3)

```

Print alpha to show that 50 percent are popular


```{r}
## 50 percent are popular
alpha


```

Now get the individual probabilities for each artist. We can print the keys that are most probable to be hits for each artist. 
```{r}

#Getting Individual probabilities for each player
finalindprops <- Estep(mean_pop,alpha,mu0,mu1,sigsq0, sigsq1)

hist(finalindprops)
sum(finalindprops > 0.90)
artists.topHR<-clean_data[finalindprops > 0.90,1:3]
artists.topHR

```


Then, for a single artist, print the probability of hit in each of the 14 keys 
```{r}


## probability of being a hit song
### Velvet Underground
id <- clean_data$artist_name
player_index <- id == "The Velvet Underground"
y <- mean_pop
ind <- curind
Estep(y[player_index], alpha, mu0, mu1, sigsq0, sigsq1)

```

**Equal Variance Model**
By forcing the two components to have the same variance, alpha becomes 0.26, in tune with our hypothesis of hit song making being a rare event.


```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}



#EM algorithm for equal-variance model
Estep2 <- function(y,alpha,mu0,mu1,sigsq){
  n <- length(y)  
  ind <- rep(NA,n)
  for (i in 1:n){
    prob0 <- (1-alpha)*dnorm(y[i],mean=mu0,sd=sqrt(sigsq))
    prob1 <- alpha*dnorm(y[i],mean=mu1,sd=sqrt(sigsq))
    ind[i] <- prob1/(prob0+prob1)
  }
  ind
}
Mstep2 <- function(y,ind){
  n <- length(y)
  alpha <- sum(ind)/n
  mu1 <- sum(ind*y)/sum(ind)
  mu0 <- sum((1-ind)*y)/sum(1-ind)
  sigsq <- sum(ind*((y-mu1)^2))
  sigsq <- sigsq+sum((1-ind)*((y-mu0)^2))
  sigsq <- sigsq/n
  c(alpha,mu0,mu1,sigsq)
}
##observed data loglikelihood function for equal variance model
loglik.mix2 <- function(y,ind,alpha,mu0,mu1,sigsq){
  loglik <- sum(log(alpha*dnorm(y,mu1,sqrt(sigsq))+(1-alpha)*dnorm(y,mu0,sqrt(sigsq))))
  loglik
}
curalpha <- 0.1
curmu0 <- 0.001
curmu1 <- 0.15
cursigsq <- 0.1
curind <- Estep2(mean_pop,curalpha,curmu0,curmu1,cursigsq)
loglik <- loglik.mix2(mean_pop,curind,curalpha,curmu0,curmu1,cursigsq)

itermat2 <- c(curalpha,curmu0,curmu1,cursigsq,loglik)
diff <- 1
numiters <- 1
while (diff > 0.001 || numiters <= 100){
  curind <- Estep2(mean_pop,curalpha,curmu0,curmu1,cursigsq)
  curparam <- Mstep2(mean_pop,curind)
  curalpha <- curparam[1]
  curmu0 <- curparam[2]
  curmu1 <- curparam[3]
  cursigsq <- curparam[4]
  loglik <- loglik.mix2(mean_pop,curind,curalpha,curmu0,curmu1,cursigsq)
  itermat2 <- rbind(itermat2,c(curparam,loglik))
  numiters <- numiters + 1
  diff <- max(abs(itermat2[numiters,]-itermat2[numiters-1,])) 
  # print (c(numiters,loglik))
}


#Tracking iterations
parametertext <- c("alpha","mu0","mu1","sigsq","loglik")
par(mfrow=c(2,3))
for (i in 1:5){
  plot(1:numiters,itermat2[,i],type="l",main=parametertext[i],xlab="Iterations",ylab="Value")
}

# plotting equal-variances fitted mixture density
finalparam<-itermat2[numiters,]
alpha <- finalparam[1]
mu0 <- finalparam[2]
mu1 <- finalparam[3]
sigsq <- finalparam[4]
par(mfrow=c(1,1))
plot2<- hist(mean_pop,prob=T, main = "MLE Densities for
             Fixed Variance")
x <- ppoints(1000)*0.6
y1 <- (1-alpha)*dnorm(x,mu0,sqrt(sigsq))
y2 <- alpha*dnorm(x,mu1,sqrt(sigsq))
lines(x,y1,col=2)
lines(x,y2,col=3)
alpha

```


**Sample Predictions**

Consider the example of Modest Mouse. The Keys of A minor, C major, D minor, E major and G major are highly likely to produce hit songs. The posterior probability of popularity for each key are mapped below. Recall that these predictions are based on the average popularity of all songs in a key. Mean popularity doesnot capture the variance within a key and between keys. Bayesian Hiearchical Modeling will do better at taking variance into account.


```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}

### Modest Mouse
id <- clean_data$artist_name
artist_index <- id == "Modest Mouse"
y <- mean_pop
ind <- curind
modestmouse_preds <- Estep2(y[artist_index], alpha, mu0, mu1, sigsq)
# individual key predictions
modestmouse_preds
modest <- clean_data %>%
  filter(artist_name == "Modest Mouse")
keys <- unique(modest$key_mode)
pop <- modest$mean_pop
EM_dataframe_modest <- data.frame(keys, modestmouse_preds, pop)
EM_dataframe_modest <- EM_dataframe_modest %>%
  rename("Posterior Prediction" = modestmouse_preds,
         "Popularity" = pop,
         "Modest Mouse" = keys) %>%
  mutate("Posterior Prediction" = (round(modestmouse_preds, 4))) %>%
  mutate("Popularity" = (round(pop, 4)))
library(kableExtra)
kbl(EM_dataframe_modest[1:8,]) %>%
  kable_paper("hover", full_width = F) %>%
  row_spec(2, bold = T) %>%
  row_spec(7, bold = T)
kbl(EM_dataframe_modest[9:16,]) %>%
  kable_paper("hover", full_width = F) %>%
  row_spec(1, bold = T) %>%
  row_spec(2, bold = T)

```

## Hierarchical Bayes

Assume a known $\sigma^2$ and model the parameters yij as the observations i in group j and mu as the global mean.

$$\begin{equation}
y =  Normal(\mu_j, \sigma^2)
\end{equation}$$


$$\begin{equation}
\mu =  Normal(\mu_o, \tau^2)
\end{equation}$$
The posterior distribution of muj within each group is:

$$\begin{equation}
p(\theta_i\mid \mu, \sigma^2,y, \tau^2) = N(\frac{\frac{1}{\sigma_i^2}y_i + \frac{1}{\tau^2}\mu}{\frac{1}{\sigma^2}+\frac{1}{\tau^2}}, \frac{1}{\frac{1}{\sigma_i^2} + \frac{1}{\tau^2}})
\end{equation}$$


```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}

####
### step 5: Hierarchical Prediction
###

### this has to be a sample: sample 5 songs from each
load("/cloud/project/data/output_bayes (2).Rdata")
merge <- new_data
merge <- merge %>%
  select(track_name, artist_name, key_mode, popularity_index)
merge <- merge %>%
  arrange(track_name, popularity_index) %>%
  filter(duplicated(track_name) == FALSE)
merge <- merge[- grep("-", merge$track_name),]
merge <- merge %>%
  arrange(track_name, popularity_index) %>%
  filter(duplicated(track_name) == FALSE)
merge <- merge[- grep("live", merge$track_name),]
merge <- merge[- grep("Live", merge$track_name),]
# merged <- merge %>%
#   group_by(artist_name, key_mode) %>%
#   select(artist_name, key_mode, popularity_index, track_name) %>%
#   filter(popularity_index >0)
### choose a sampling procedure for this
df <-merge %>%
  filter(artist_name == "Johnny Cash") %>%
  select(key_mode, popularity_index, artist_name) %>%
  group_by(key_mode) %>%
  filter(popularity_index > 0)
df <- df %>% 
  group_by_at(vars(-popularity_index)) %>%  # g
  mutate(row_id=1:n()) %>%
  ungroup() %>%
  spread(key_mode, popularity_index)
## we had to choose keys with at least a 10 song representation in the dataset
johnnycash <- df %>%
  select(artist_name, "A major", "A minor", "B major", "B minor",
         "C major", "C# major", "C minor", "D major", "D# major", "E major",
         "E minor", "F major", "F# major", "G major", "G# major")
johnnycash <- johnnycash[1:10,2:16]

### different variances in each
var(johnnycash$`A minor`)
var(johnnycash$`B minor`)

###HLM Prediction
y_johnnycash <- data.frame(lapply(johnnycash, function(x) as.numeric(as.character(x))))
dat <- stack(as.data.frame(y_johnnycash))
library(ggplot2)
plot1<- ggplot(dat) + 
  geom_boxplot(aes(x = ind, y = values)) +
  ylab("Popularity") +
  xlab("")+
  theme(axis.title.x = element_blank(),
        axis.ticks.x= element_blank()) +
  theme_minimal() +
  ggtitle("Johnny Cash") +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))

#### Animal Collective

df2 <-merge %>%
  filter(artist_name == "Animal Collective") %>%
  select(key_mode, popularity_index, artist_name) %>%
  group_by(key_mode)
df2 <- df2 %>% 
  group_by_at(vars(-popularity_index)) %>%  # g
  mutate(row_id=1:n()) %>%
  ungroup() %>%
  spread(key_mode, popularity_index)
animal_collective <- df2 %>%
  select(artist_name, "A major", "A minor", "B major",
         "C major", "C# major", "C minor", "D major", "D# major", "E major",
         "E minor", "F major", "F# major", "G major", "G# major")
animal_collective <- animal_collective[1:3,2:15]
y_animal_collective <- data.frame(lapply(animal_collective, function(x) as.numeric(as.character(x))))
y_animal_collective

### different variances in each
var(y_animal_collective$C..major)

###HLM Prediction
y_animal_collective <- data.frame(lapply(y_animal_collective, function(x) as.numeric(as.character(x))))
dat <- stack(as.data.frame(y_animal_collective))
library(ggplot2)
plot2<- ggplot(dat) + 
  geom_boxplot(aes(x = ind, y = values)) +
  ylab("Popularity") +
  xlab("")+
  theme(axis.title.x = element_blank(),
        axis.ticks.x= element_blank()) +
  theme_minimal() +
  ggtitle("Animal Collective") +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))

library(ggpubr)
library(gridExtra)
ggarrange(plot1, plot2)
```



Next step
