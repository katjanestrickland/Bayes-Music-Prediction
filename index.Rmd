---
title: "Hierarchical Bayes Prediction"
author: "Kat Wilson"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Overview

Bob Dylan doesn't often grace the pages of the Wall Street Journal, but last year, the iconic folk singer made financial headlines when he sold the entirety of his music catalog for 300 million dollars. Songs, like other forms of art, are a potentially valuable yet hard to appraise investment: valuable because of the potential to build more popularity from modern streaming services, and all the more uncertain because of the inherent ambiguity in predicting how creative leaps will be adored or ignored by music fans.

Data from streaming platforms offers some hope in modelling future cash flows from new releases, and this modelling begins with prediction of new songs' popularity on streaming platforms. In this analysis, I approach the question from a Bayesian perspective. The prediction centers around how a song written in one of the 12 keys will perform in popularity count on Spotify, the largest worldwide streaming platform. Bayesian methods are particularly suited to model uncertainty, and perform well by incorporating prior information built into our vast dataset of songs. They perform even better when we model this information hierarchically. An analysis of this type might appeal to the creative types- what makes music 'experimental?'- yet also satisfy a financial itch- is a new song in a new key going to sell?

## Intro to the Spotify API - Keys and Popularity Index

Using the Spotify API, I pulled the discographies of 15 bands that cross range of musical style. Data includes 40 measured variables per track, such as album name, track name, duration, and tempo.


```{r fig.height=5, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}
library(tidyverse)
load("/cloud/project/data/animal_chapter_data.Rdata")
load("/cloud/project/data/output_bayes (2).Rdata")
merge <- new_data
colnames(merge)
table(merge$artist_name)

```

"*Every art fights the noose of verbal description*", writes Alex Ross, the New Yorker music critic, in his expansive music history,"The Rest is Noise". Ross has done better than anyone at attempting to describe the art form of music, yet still admits the difficulty of evaluating this deep and difficult art. However challenging it might be to pin down musical creativity, he still admits that song structure is founded on certain building blocks, and that understanding these basics can help us to try and wrangle in the elusive concept that is creativity.

One of these building blocks are the keys. A key is the pattern of intervals between notes, from half tone to semi tone (think of the jump from a black to a white key on a keyboard).  Keys are named with letter names, names that denote the tonic, or the center, of that key. The key of A for instance, is centered around the A tonic, and the key of F is centered around the F tonic.

Although the advent of equal temperament tuning has largely put key differences to rest, there still might be something to our expectation in keys. Although we can now transpose any song into any key, especially to match the singer's intended tone or the intended song timbre, artists still write across the spectrum. A diversity of song keys is likely to make a more appealing album, as songs in different keys span the mood spectrum and demonstrate more creativity, or stick out from the crowd. The reason why like our favorite bands is precisely because they take risks with different keys. To the non-thinking ear, we are drawn to the spectrum of audible emotion. 

Looking at the count of raw data, the distribution of keys looks similar to what others have determined to be the most frequented keys on Spotify. The data sample's distribution reflects the distribution of all keys on Spotify: keys of G major, D major and C major (more simple keys with less sharps and flats) are represented the most. 

The distribution of keys doesn't hold necessarily across artists, however. The experimental band Radiohead wrote more songs in the key of Asharp than Animal Collective, for instance, who write more songs in the Key of D. This begs the question of how hits are formed among artist. Does a band get better by writing a lot in one key? Could other bands be more likely to land on a hit when they write outside of their most frequented key? The distribution of popularity and keys also don't hold across artists. In addition to inspecting how frequently bands write in a certain key, we can also investigate how popular songs in a certain key are for each band.

```{r fig.height=5, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}
par(mfrow=c(1,1))
clean_data_count <- new_data %>%
  group_by(key_mode) %>%
  mutate(value = n()) %>%
  select(key_mode, mode_name, value) %>%
  na.omit()
clean_data_count$mode_name <- as.factor(clean_data_count$mode_name)
clean_data_count$key_mode <- as.factor(clean_data_count$key_mode)
ggplot(clean_data_count, aes(x = key_mode,
                             fill = mode_name, y = value)) +
  geom_bar(stat = "identity",position = "dodge") +
  theme(axis.text.x = element_text(angle = 40),
        legend.title = element_blank(),
        panel.background = element_blank())+
  ylab("Count") +
  xlab("") +
  scale_fill_manual("legend", values = c("major" = "salmon",
                                         "minor" = "grey"))



```

Spotify's popularity index will give us more information than sheer key count. The popularity index is a number from 0-100 that represents the total number of plays a track has. To investigate how popularity varies with key usage, we only need the artist name, track name, key and popularity index. Let's also remove Live tracks, as the popularity of a live version might conflate the original studio recorded version. A summary statistic (the mean popularity) for each artists' key is computed, from these tracks, and converted to a decimal (probability).

```{r fig.height=5, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}
merge <- merge %>%
  select(track_name, artist_name, key_mode, popularity_index)
merge <- merge %>%
  arrange(track_name, popularity_index) %>%
  filter(duplicated(track_name) == FALSE)
merge <- merge[- grep("-", merge$track_name),]
merge <- merge[- grep("live", merge$track_name),]
merge <- merge[- grep("Live", merge$track_name),]
merged_new<- merge %>%
  group_by(artist_name, key_mode) %>%
  select(artist_name, key_mode, popularity_index, track_name) %>%
  filter(popularity_index >0)
merged_new$popularity_index <- as.numeric(merged_new$popularity_index)
merged_new <- merged_new %>%
  group_by(artist_name, key_mode) %>%  
  mutate(n = n()) %>%
  ungroup() %>%
  group_by(artist_name,key_mode)%>%
  summarise(mean_pop = mean(popularity_index)) %>%
  mutate(mean_pop = mean_pop/100)
head(merged_new)
```
Next, we can visualize popularity across keys for this group of artists. Does listener demand or artist intend drive the interaction between key popularity and usage? Maybe Nirvana fans just more "down". In that case, the Eminor key would be more popular than the Eminor key among Beyonce fans? Nirvana also wrote more songs in the keys of C and Gmajor than any other key. However, their most popular songs are in more experimental keys, such as D#minor, Fminor, and E minor. This begs the question of what we come to expect from a band, and what turns out to be popular among fans. The Velvet Underground only has one song in Fminor, but the song (Sunday Morning, off of Velvet Underground and Nico) is their most popular. The Velvet Underground wrote the most songs in the key of Dmajor, and there is not much variability in this key. Could it be because they are less inclined to take risks in that key? The final section will touch on this concept of hierarchical variance. First, though, let’s use Bayesian prediction to predict hit tracks.

```{r fig.height=5, fig.width=7, message=FALSE, warning=FALSE, include=FALSE}
### aggregated data
aggregated_data <- merged_new
save(aggregated_data, file = "/cloud/project/data/aggregated_data.Rdata")
## only Velvet Underground
load("/cloud/project/data/total_data.Rdata")
merged_new<- merged_new %>%
  filter(artist_name == "The Velvet Underground") %>%
  group_by(artist_name, key_mode) %>%
  select(artist_name, key_mode, popularity_index) %>%
  filter(popularity_index >0)
merged_new$popularity_index <- as.numeric(merged_new$popularity_index)
merged_new <- merged_new %>%
  group_by(artist_name, key_mode) %>%  
  mutate(n = n()) %>%
  ungroup() %>%
  group_by(artist_name,key_mode, n) %>%
  summarise(mean_pop = popularity_index) %>%
  mutate(mean_pop = mean_pop/100)
velvet_underground<- merged_new
save(velvet_underground, file = "/cloud/project/data/velvet_underground.Rdata")


## only Beyonce
load("/cloud/project/data/total_data.Rdata")
merged_new<- merged_new %>%
  filter(artist_name == "Beyoncé") %>%
  group_by(artist_name, key_mode) %>%
  select(artist_name, key_mode, popularity_index) %>%
  filter(popularity_index >0)
merged_new$popularity_index <- as.numeric(merged_new$popularity_index)
merged_new <- merged_new %>%
  group_by(artist_name, key_mode) %>%  
  mutate(n = n()) %>%
  ungroup() %>%
  group_by(artist_name,key_mode, n) %>%
  summarise(mean_pop = popularity_index) %>%
  mutate(mean_pop = mean_pop/100)
beyonce<- merged_new
save(beyonce, file = "/cloud/project/data/beyonce.Rdata")

## only Cash
load("/cloud/project/data/total_data.Rdata")
merged_new<- merged_new %>%
  filter(artist_name == "Johnny Cash") %>%
  group_by(artist_name, key_mode) %>%
  select(artist_name, key_mode, popularity_index) %>%
  filter(popularity_index >0)
merged_new$popularity_index <- as.numeric(merged_new$popularity_index)
merged_new <- merged_new %>%
  group_by(artist_name, key_mode) %>%  
  mutate(n = n()) %>%
  ungroup() %>%
  group_by(artist_name,key_mode, n) %>%
  summarise(mean_pop = popularity_index) %>%
  mutate(mean_pop = mean_pop/100)
cash <- merged_new
save(cash, file = "/cloud/project/data/cash.Rdata")


load("/cloud/project/data/aggregated_data.Rdata")
set <- as.data.frame(table(merged_new$artist_name, merged_new$key_mode))
load("/cloud/project/data/velvet_underground.Rdata")
load("/cloud/project/data/beyonce.Rdata")
load("/cloud/project/data/cash.Rdata")
clean_data <- merged_new

par(mfrow=c(2,2))
####
### step 2: visualize popularity
####
library(ggpubr)
library(gridExtra)
clean_data <-
  clean_data %>%
  mutate(key_mode = fct_reorder(key_mode, mean_pop))
## only Vevlet
velvet_underground <- velvet_underground %>%
  filter(artist_name == "The Velvet Underground")
## only Beyoncé
Beyoncé <- beyonce %>%
  filter(artist_name == "Beyoncé")
## only Cash
cash <- cash %>%
  filter(artist_name == "Johnny Cash")

```

```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}
'All Data' <- aggregated_data
'Velvet Underground' <- velvet_underground
'Beyoncé' <- beyonce
'Johnny Cash' <- cash
df_list <- list(`All Data`, `Velvet Underground`, `Beyoncé`, `Johnny Cash`)
for (i in 1:length(df_list)) {
  data <- as.data.frame(df_list[i])
b<- ggplot(data = data, aes(x= reorder(factor(key_mode), mean_pop), y = mean_pop)) +
  stat_summary(fun.data = mean_se) +
  coord_flip() +
  ggtitle(as.character(data[1,1]))+
  ylab("Mean Popularity Score") +
  xlab(NULL) +
  theme_minimal()
assign(paste("plot", as.character(data[1,1])), b)
}
ggarrange(ggarrange(`plot Allman Brothers Band`, `plot Beyoncé`,
                    `plot Johnny Cash`, `plot The Velvet Underground`))


```


## Bayesian Modeling Approach

I fit a two-component mixture model to the distribution of popularity averages, one that represents songs with average popularity, and another that represent hit tracks. We don't observe the grouping of songs in the data generation process (i.e. Spotify doesn't have a 'hit track' variable); therefore, this is considered a "missing data" problem. The Expectation-Maximization (EM) algorithm will help to identify which of the two components the data came from. Furthermore, since the distrubtion of popularity appears to be normal, then we choose a normal-normal mixture model.

```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}
load("/cloud/project/data/aggregated_data.Rdata")
clean_data <- aggregated_data
#Calculating mean_pop
clean_data$mean_pop <- as.numeric(clean_data$mean_pop)
mean_pop <- clean_data$mean_pop
hist(mean_pop)
# artist <-  clean_data$artist_name
# key_mode <- clean_data$key_mode
```



$$\begin{equation}
y =  Normal(\mu, \sigma^2)
\end{equation}$$

$$\begin{equation}
y =  Normal(\mu, \sigma^2)
\end{equation}$$

The model consists of 5 variance components: the mu (means) of the first two groups, the sigma squares (variances) of the two groups, and the alpha, representing the probability that any particular data point (song) is from the 1 component. A point from the 0 component is represented by 1-alpha. The five unknown parameters are given by

$$\begin{equation}
\theta = (\mu_1, \mu_0, \sigma_1^2, \sigma_0^2, \alpha)
\end{equation}$$

Estimating theta is difficult because we don't know which of the two components each observation came from. If we had the missing data, we could seperate these units into their respective components, but instead, we'll consider the missing data as an indicator. Each unit (song) has an indicator, equal to 1 if the observation (that song) came from group 1, and equal to 0 if the observation came from group 0. Alpha is the proportion of observations that are in group 1 and 0. In a counter-intuitive concept to much of statistical modeling, adding more "carefully chosen parameters" (i.e. these indicators) improves model estimation.

The likelihood for the EM is written as a function of both the observed data and the missing data, where Y is the observed data, I is missing data. Observed and missing data are both conditioned on theta, as opposed to only observed variables conditioned on theta, which is the observed data likelihood.

$$\begin{equation}
P(Y, I | \theta)
\end{equation}$$

We can't use the actual values (i) as the indicators are unobservable. Instead, we'll plug in their expected values (yhat) to the actual likelihood. The expected value of an indicator becomes the probability of that observation (song) being in group 1. 

EM follows with two steps. To begin, with the E step, the expected indicator for every single observation (whether the observation is in the popular or un-popular group) is calculated. Next, in the M step, we take these indicator values from the E step and outputs new values for each parameters. The E step takes in current parameter values and outputs estimated values for the indicators. The M step takes in values for the indicators and outputs values for the parameters. The point of the M step is to give better values for the E step, where every step improves the estimates of the unknown. As a hill climbing algorithm, EM improves the estimates until it finds a local mode.

**Expectation Step**

E step takes in the current parameter values:

```{r fig.height=4, fig.width=4, message=FALSE, warning=FALSE, include=TRUE}
#Expectation function
Estep <- function(y,alpha,mu0,mu1,sigsq0,sigsq1){
  n <- length(y)  
  ind <- rep(NA,n)
  for (i in 1:n){
    prob0 <- (1-alpha)*dnorm(y[i],mean=mu0,sd=sqrt(sigsq0))
    prob1 <- alpha*dnorm(y[i],mean=mu1,sd=sqrt(sigsq1))
    ind[i] <- prob1/(prob0+prob1)
  }
  ind
}
```

**Maximization Step**

M step otputs new values for the parametres, and estimates the parameters given the indicators:

```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}
#Maximization function
Mstep <- function(y,ind){
  n <- length(y)
  alpha <- sum(ind)/n
  mu1 <- sum(ind*y)/sum(ind)
  mu0 <- sum((1-ind)*y)/sum(1-ind)
  sigsq1 <- sum(ind*((y-mu1)^2))/sum(ind)
  sigsq0 <- sum((1-ind)*((y-mu0)^2))/sum(1-ind)
  c(alpha,mu0,mu1,sigsq0,sigsq1)
}
```

Choose an arbitrary starting point (curalpha as 0.5), and run the while loop for 100 iterations.
```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}
##observed data log likelihood function
loglik.mix <- function(y,ind,alpha,mu0,mu1,sigsq0,sigsq1){
  loglik <- sum(log(alpha*dnorm(y,mu1,sqrt(sigsq1))+(1-alpha)*dnorm(y,mu0,sqrt(sigsq0))))
  loglik
}
```


In this code sample, I comment out the iteration values, which would otherwise show the iteration plus observed data likelihood at each step. The likelihood improves until it reaches the top of the hill. 

Plots for the course of the parameters is mapped below. Most of the gains in the observed data likelihood appear in the first few iterations, and after that, parameter estiamtes flatten out until the model converges. Estimates result in an overall maximum log-likelihood (=296.61) with mu0 at 0.18, mu1 at .40., and alpha at 0.40. The optimal kind of mixture components for this distribution is further mapped on to the MLE Densities histogram, where the green distribution represents hit tracks, and the red distribution, non-hit tracks.

This model assigns a high probability to a song being a hit (at .40). Notice the overlap in distributions, as well. Songs that never reach popularity over 0.2 still have a probability of being in the hit track group. The flexibility built into the model (from assigning unique variances per group) causes this overlap. The green group (hit songs) doesn't just have a higher mean, it also has a greater variance. Next, we constrain the variances to address the overlap.

```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}

#Running EM iterations
curalpha <- 0.5
curmu0 <- 0.1
curmu1 <- 0.4
cursigsq0 <- 0.03
cursigsq1 <- 0.03
mean(mean_pop)
curind <- Estep(mean_pop,curalpha,curmu0,curmu1,cursigsq0,cursigsq1)
loglik <- loglik.mix(mean_pop,curind,curalpha,curmu0,curmu1,cursigsq0,cursigsq1)
itermat <- c(curalpha,curmu0,curmu1,cursigsq0,cursigsq1,loglik)
diff <- 1
numiters <- 1
while (diff > 0.001 || numiters <= 100){
  curind <- Estep(mean_pop,curalpha,curmu0,curmu1,cursigsq0,cursigsq1)
  curparam <- Mstep(mean_pop,curind)
  curalpha <- curparam[1]
  curmu0 <- curparam[2]
  curmu1 <- curparam[3]
  cursigsq0 <- curparam[4]
  cursigsq1 <- curparam[5]
  itermat <- rbind(itermat,c(curparam,loglik))
  loglik <- loglik.mix(mean_pop,curind,curalpha,curmu0,curmu1,cursigsq0,cursigsq1)
  numiters <- numiters + 1
  diff <- max(abs(itermat[numiters,]-itermat[numiters-1,])) 
  # print (c(numiters,loglik))
}
parametertext <- c("alpha","mu0","mu1","sigsq0","sigsq1","loglik")
par(mfrow=c(2,3))
for (i in 1:6){
  plot(1:numiters,itermat[,i],main=parametertext[i],xlab="Iterations",ylab="Value")
}
# plotting fitted mixture density
finalparam<-itermat[numiters,]
alpha <- finalparam[1]
mu0 <- finalparam[2]
mu1 <- finalparam[3]
sigsq0 <- finalparam[4]
sigsq1 <- finalparam[5]
par(mfrow=c(1,1))

plot1<- hist(mean_pop,prob=T, main= "MLE Densities for Unique Variances")
x <- ppoints(1000)*0.9
y1 <- (1-alpha)*dnorm(x,mu0,sqrt(sigsq0))
y2 <- alpha*dnorm(x,mu1,sqrt(sigsq1))
## mu 0 is 0.07, and mu1 is .32
lines(x,y1,col=2)
lines(x,y2,col=3)

```

Hit rate of 40% is too high. Fixing the variances equal for both groups will constrain this variability.

```{r}
## 40 percent are popular
alpha

```

Individual probabilities for each artist's key are estimated from the parameters saved in the Estep. The histogram shows likelihood of all songs in the hit or not hit group. Keys are either likely to be in group 1, or not. 
```{r}

#Getting Individual probabilities for each player
finalindprops <- Estep(mean_pop,alpha,mu0,mu1,sigsq0, sigsq1)

hist(finalindprops)

```



We can also identify the keys for each artist with higher than 90 percent chance of being a hit. Animal Collective has 4 keys with higher than a 90% chance of being a hit. Although these songs are lower average popularity than other artists, they are, for animal collective, the highest likely songs to be hits.

```{r}
sum(finalindprops > 0.90)
artists.topHR<-clean_data[finalindprops > 0.90,1:3]
artists.topHR

```
For a single artist, we can also pull the probability of a hit in each of the 14 keys. Fminor, below, is the key with 100% chance of being a hit. The probability is blown up because, as discussed above, Velvet Underground's only Fminor song is Sunday Morning. 
```{r}
### Velvet Underground
id <- clean_data$artist_name
player_index <- id == "The Velvet Underground"
y <- mean_pop
ind <- curind
Estep(y[player_index], alpha, mu0, mu1, sigsq0, sigsq1)

```

**Equal Variance Model**

Forcing the two components to have the same variance gives a slighly different solution. Alpha becomes 0.26, in tune with our hypothesis of hit song making being a rare event. The vast majority of songs are not hits. When we pull out the artist keys that have greater than .90 probability of a hit, these are more reliable estimates.


```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}

#EM algorithm for equal-variance model
Estep2 <- function(y,alpha,mu0,mu1,sigsq){
  n <- length(y)  
  ind <- rep(NA,n)
  for (i in 1:n){
    ## notice that sigsq is the same for each component
    prob0 <- (1-alpha)*dnorm(y[i],mean=mu0,sd=sqrt(sigsq))
    prob1 <- alpha*dnorm(y[i],mean=mu1,sd=sqrt(sigsq))
    ind[i] <- prob1/(prob0+prob1)
  }
  ind
}
Mstep2 <- function(y,ind){
  n <- length(y)
  alpha <- sum(ind)/n
  mu1 <- sum(ind*y)/sum(ind)
  mu0 <- sum((1-ind)*y)/sum(1-ind)
  sigsq <- sum(ind*((y-mu1)^2))
  sigsq <- sigsq+sum((1-ind)*((y-mu0)^2))
  sigsq <- sigsq/n
  c(alpha,mu0,mu1,sigsq)
}
##observed data loglikelihood function for equal variance model
loglik.mix2 <- function(y,ind,alpha,mu0,mu1,sigsq){
  loglik <- sum(log(alpha*dnorm(y,mu1,sqrt(sigsq))+(1-alpha)*dnorm(y,mu0,sqrt(sigsq))))
  loglik
}
curalpha <- 0.1
curmu0 <- 0.001
curmu1 <- 0.15
cursigsq <- 0.1
curind <- Estep2(mean_pop,curalpha,curmu0,curmu1,cursigsq)
loglik <- loglik.mix2(mean_pop,curind,curalpha,curmu0,curmu1,cursigsq)

itermat2 <- c(curalpha,curmu0,curmu1,cursigsq,loglik)
diff <- 1
numiters <- 1
while (diff > 0.001 || numiters <= 100){
  curind <- Estep2(mean_pop,curalpha,curmu0,curmu1,cursigsq)
  curparam <- Mstep2(mean_pop,curind)
  curalpha <- curparam[1]
  curmu0 <- curparam[2]
  curmu1 <- curparam[3]
  cursigsq <- curparam[4]
  loglik <- loglik.mix2(mean_pop,curind,curalpha,curmu0,curmu1,cursigsq)
  itermat2 <- rbind(itermat2,c(curparam,loglik))
  numiters <- numiters + 1
  diff <- max(abs(itermat2[numiters,]-itermat2[numiters-1,])) 
  # print (c(numiters,loglik))
}

#Tracking iterations
parametertext <- c("alpha","mu0","mu1","sigsq","loglik")
par(mfrow=c(2,3))
for (i in 1:5){
  plot(1:numiters,itermat2[,i],type="l",main=parametertext[i],xlab="Iterations",ylab="Value")
}

# plotting equal-variances fitted mixture density
finalparam<-itermat2[numiters,]
alpha <- finalparam[1]
mu0 <- finalparam[2]
mu1 <- finalparam[3]
sigsq <- finalparam[4]
par(mfrow=c(1,1))
plot2<- hist(mean_pop,prob=T, main = "MLE Densities for
             Fixed Variance")
x <- ppoints(1000)*0.6
y1 <- (1-alpha)*dnorm(x,mu0,sqrt(sigsq))
y2 <- alpha*dnorm(x,mu1,sqrt(sigsq))
lines(x,y1,col=2)
lines(x,y2,col=3)
alpha

```


**Sample Predictions**

Consider the example of Modest Mouse. The Keys of C# major, E major and G# major are highly likely (over .90 posterior probability) to produce hit songs. The posterior probability of popularity for each key are mapped below. In a valuation sense, these posterior predictions and related predictive intervals are more credible to uncertainty compared to a regular confidence interval.

Recall, however, that these posterior predictions are based on the average popularity of all songs in a key. Mean popularity does not capture the variance within a key and between keys. Bayesian Hiearchical Modeling will do better at taking variance into account.


```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE, include=TRUE}

### Modest Mouse
id <- clean_data$artist_name
artist_index <- id == "Modest Mouse"
y <- mean_pop
ind <- curind
modestmouse_preds <- Estep2(y[artist_index], alpha, mu0, mu1, sigsq)
# individual key predictions
modestmouse_preds
modest <- clean_data %>%
  filter(artist_name == "Modest Mouse")
keys <- unique(modest$key_mode)
pop <- modest$mean_pop
EM_dataframe_modest <- data.frame(keys, modestmouse_preds, pop)
EM_dataframe_modest <- EM_dataframe_modest %>%
  rename("Posterior Prediction" = modestmouse_preds,
         "Popularity" = pop,
         "Modest Mouse" = keys) %>%
  mutate("Posterior Prediction" = (round(modestmouse_preds, 4))) %>%
  mutate("Popularity" = (round(pop, 4)))
library(kableExtra)
kbl(EM_dataframe_modest[1:8,]) %>%
  kable_paper("hover", full_width = F) %>%
  row_spec(2, bold = T) %>%
  row_spec(7, bold = T)
kbl(EM_dataframe_modest[9:16,]) %>%
  kable_paper("hover", full_width = F) %>%
  row_spec(1, bold = T) %>%
  row_spec(2, bold = T)

```

## Hierarchical Bayes

Notice substantial variation between each key within each artist. Our first Bayesian model assumed that the mean is the same in each key, but hiearchical bayes will allow us to have a different data situation for each key. Staying in the normal family, we think about what parameters are shared or not shared between and within groups. 

```{r echo=FALSE, fig.height=6, fig.width=7, message=FALSE, warning=FALSE}

### this has to be a sample: sample 5 songs from each
load("/cloud/project/data/output_bayes (2).Rdata")
merge <- new_data
merge <- merge %>%
  select(track_name, artist_name, key_mode, popularity_index)
merge <- merge %>%
  arrange(track_name, popularity_index) %>%
  filter(duplicated(track_name) == FALSE)
merge <- merge[- grep("-", merge$track_name),]
merge <- merge %>%
  arrange(track_name, popularity_index) %>%
  filter(duplicated(track_name) == FALSE)
merge <- merge[- grep("live", merge$track_name),]
merge <- merge[- grep("Live", merge$track_name),]
# merged <- merge %>%
#   group_by(artist_name, key_mode) %>%
#   select(artist_name, key_mode, popularity_index, track_name) %>%
#   filter(popularity_index >0)
### choose a sampling procedure for this
df <-merge %>%
  filter(artist_name == "Johnny Cash") %>%
  select(key_mode, popularity_index, artist_name) %>%
  group_by(key_mode) %>%
  filter(popularity_index > 0)
df <- df %>% 
  group_by_at(vars(-popularity_index)) %>%  # g
  mutate(row_id=1:n()) %>%
  ungroup() %>%
  spread(key_mode, popularity_index)
## we had to choose keys with at least a 10 song representation in the dataset
johnnycash <- df %>%
  select(artist_name, "A major", "A minor", "B major", "B minor",
         "C major", "C# major", "C minor", "D major", "D# major", "E major",
         "E minor", "F major", "F# major", "G major", "G# major")
johnnycash <- johnnycash[1:10,2:16]

# ### different variances in each
# var(johnnycash$`A minor`)
# var(johnnycash$`B minor`)

###HLM Prediction
y_johnnycash <- data.frame(lapply(johnnycash, function(x) as.numeric(as.character(x))))
dat <- stack(as.data.frame(y_johnnycash))
library(ggplot2)
plot1<- ggplot(dat) + 
  geom_boxplot(aes(x = ind, y = values)) +
  ylab("Popularity") +
  xlab("")+
  theme(axis.title.x = element_blank(),
        axis.ticks.x= element_blank()) +
  theme_minimal() +
  ggtitle("Johnny Cash") +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))

#### Animal Collective

df2 <-merge %>%
  filter(artist_name == "Animal Collective") %>%
  select(key_mode, popularity_index, artist_name) %>%
  group_by(key_mode)
df2 <- df2 %>% 
  group_by_at(vars(-popularity_index)) %>%  # g
  mutate(row_id=1:n()) %>%
  ungroup() %>%
  spread(key_mode, popularity_index)
animal_collective <- df2 %>%
  select(artist_name, "A major", "A minor", "B major",
         "C major", "C# major", "C minor", "D major", "D# major", "E major",
         "E minor", "F major", "F# major", "G major", "G# major")
animal_collective <- animal_collective[1:3,2:15]
y_animal_collective <- data.frame(lapply(animal_collective, function(x) as.numeric(as.character(x))))
# y_animal_collective

# ### different variances in each
# var(y_animal_collective$C..major)

###HLM Prediction
y_animal_collective <- data.frame(lapply(y_animal_collective, function(x) as.numeric(as.character(x))))
dat <- stack(as.data.frame(y_animal_collective))
library(ggplot2)
plot2<- ggplot(dat) + 
  geom_boxplot(aes(x = ind, y = values)) +
  ylab("Popularity") +
  xlab("")+
  theme(axis.title.x = element_blank(),
        axis.ticks.x= element_blank()) +
  theme_minimal() +
  ggtitle("Animal Collective") +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))

library(ggpubr)
library(gridExtra)
ggarrange(plot1, plot2)
```




One observation for song i in group (key) j is represented by Yij, where yijs are normally distribetd within each group. N(mu, sigma), allows a different mean for every single group. Muj is the "key effect", where the variance in the key is attributable to that key (certain keys are more popular than others).

If we analyzed each key separately, that would assume that each key has nothing to do with one another. This model, however, allows us the parameterize the actual variation between each key which is shared information between the keys (due to artist affect). The shared information is reflected in the common prior on each key within each artist. Assuming a common prior for the group specific means, where each of the key effects are normally distributed around some global overall mean for the artist. Tau squared represents variation across keys, and sigma squared represents the variance of songs within that key. We are, through this approach, modeling two levels of uncertainty in the group data structure.

Assume a known $\sigma^2$ and model the parameters yij as the observations i in group j and mu as the global mean:

$$\begin{equation}
y =  Normal(\mu_j, \sigma^2)
\end{equation}$$


$$\begin{equation}
\mu =  Normal(\mu_o, \tau^2)
\end{equation}$$
The posterior distribution of muj within each group becomes:

$$\begin{equation}
p(\theta_i\mid \mu, \sigma^2,y, \tau^2) = N(\frac{\frac{1}{\sigma_i^2}y_i + \frac{1}{\tau^2}\mu}{\frac{1}{\sigma^2}+\frac{1}{\tau^2}}, \frac{1}{\frac{1}{\sigma_i^2} + \frac{1}{\tau^2}})
\end{equation}$$

Notice two overall effects: Tau squared is the between key variation, and Mu naught is the overall artist popularity mean. Conditioned on knowing the values of the group means, j, the data are conditionally independent from those global parameters. So, all of the information on the global level is encapsulated on the other j's. Hiearchical models take advantage of such conditioning. 

The usual prior that we would use (a noninformative Jeffrey's prior) might actually look influential in this case. With two levels of unknown parameters, and only a certain amount of data to estimate, then the priors that used to be weak, or uninformative, end up now being influential. The observed data is not as strong as it used to be, and furthermore, the data signal is fairly weak on the global parameters.

When calculating the posterior distributions for each key, we also note extremes of Tau squared. As Tau squared goes to infinity, then there is infinite variation between keys. In that case, we might as well model all of the keys seperately, getting the key specific effect for each. In that case, we'd center Muj at the data, analyzing every key seperately. On the other end of the extreme, Tau squared goes to 0, and this means that there is almost no variation between keys, so every key has the same effect. In reality, Tau squared will be somewhere not right at 0, or close to infinity It is a compromize between the group specific and the global mean.

In our situation, sigma squared is known, and we will estimate the posterior by getting samples from the posterior, where the sufficient stats are just the group means. The code takes 1000 samples from the joint posterior. For each of these samples, we first sample Tau squared from the grid, and that Tau squared, loop through a sample of each of the group specific means. Small values of Tau squared, indicate pulling of the group specific mean towards a global mean (shared information between keys).


```{r echo=FALSE, fig.height=6, fig.width=7, message=FALSE, warning=FALSE}
y <- y_animal_collective
par(mfrow=c(1,1))
##Calculating necessary statistics:
m <- length(y[1,])
n <- rep(NA,m)
means <- rep(NA,m)
for (i in 1:m){
  n[i] <- length(y[,i])
  means[i] <- mean(y[,i])
}
ntot <- sum(n)

## true sigmasq
truesigsq <- 50

#####################################################
# Sampling Parameters for Normal Hierarchical Model #
#####################################################

## finding right grid for tausq
tausq.grid <- ppoints(1000)*20

tausq.logpostfunc <- function(tausq){
	Vmu0 <- 1/sum(1/(tausq + truesigsq/n))
	mu0hat <- sum(means/(tausq + truesigsq/n))*Vmu0
    out <- -0.5*log(tausq)+0.5*log(Vmu0)
    for (group in 1:m){
    	out <- out - 0.5*log(tausq + truesigsq/n[group])
    }
    for (group in 1:m){
    	out <- out - 0.5*((means[group]-mu0hat)^2)/(tausq + truesigsq/n[group])
    }
    out
}
tausq.logpost <- rep(NA,1000)
for (i in 1:1000){
	tausq.logpost[i] <- tausq.logpostfunc(tausq.grid[i])
}
tausq.post <- exp(tausq.logpost-max(tausq.logpost))
tausq.post <- tausq.post/sum(tausq.post)

par(mfrow=c(1,1))
# plot(tausq.grid,tausq.post,type="l")


numsamp <- 1000
tausq.samp <- rep(NA,numsamp)
mu0.samp <- rep(NA,numsamp)
mu.samp <- matrix(NA,nrow=numsamp,ncol=m)
for (i in 1:numsamp){
    # sampling tausq from grid of values
   	curtausq <- sample(tausq.grid,size=1,prob=tausq.post)
    # sampling mu0 given curtausq	
    Vmu0 <- 1/sum(1/(curtausq + truesigsq/n))
	mu0hat <- sum(means/(curtausq + truesigsq/n))*Vmu0
   	curmu0 <- rnorm(1,mean=mu0hat,sd=sqrt(Vmu0))
   	# sampling group means given curtausq and curmu0
   	curmu <- rep(NA,m)
   	for (j in 1:m){
   		curvar <- 1/(n[j]/truesigsq + 1/curtausq)
   		curmean <- (means[j]*n[j]/truesigsq + curmu0/curtausq)*curvar
   		curmu[j] <- rnorm(1,mean=curmean,sd=sqrt(curvar))
   	}
   	tausq.samp[i] <- curtausq
    mu0.samp[i] <- curmu0
    mu.samp[i,] <- curmu
    # print (i)
}
```


## Examining Model Parameters

```{r echo=TRUE, fig.height=6, fig.width=7, message=FALSE, warning=FALSE}
par(mfrow=c(2,2))
hist(tausq.samp,main="tausq")
hist(mu0.samp,main="mu0")
hist(mu.samp[,1],main="mu group 1")
hist(mu.samp[,2],main="mu group 2")
```
A posterior comparison between keys. Posterior probability that group 5 is greater than group 6, that group 2 greater than group 1:

```{r echo=TRUE, fig.height=6, fig.width=7, message=FALSE, warning=FALSE}
par(mfrow=c(2,2))
# posterior probability group 5 has greater mean than group 6
postprob <- sum(mu.samp[,5] > mu.samp[,6])/numsamp
postprob

# posterior probability group 2 has greater mean than group 1
postprob <- sum(mu.samp[,2] > mu.samp[,1])/numsamp
postprob
```

Using this common prior distribution, and sharing between groups, the estimates of the within group parameters are shrunk towards the global mean. The black dots are the data means, and estimates (red dots) are pulled in. For group 5, the key looks popular, but the red means not as popular as we might think. Keys that are closer to the global mean don't move as much. Extremes are pulled more.


```{r echo=TRUE, fig.height=6, fig.width=7, message=FALSE, warning=FALSE}
## shrinkage

datameans <- apply(y,2,mean)
postmeans <- apply(mu.samp,2,mean)
mu0.mean <- mean(mu0.samp)

par(mfrow=c(1,1))
plot(1:14,datameans,main="Shrinkage of Normal Means",pch=19)
abline(h=mu0.mean,col=4,lwd=2)
points(1:14,postmeans,pch=19,col=2)
legend(8,2,c("Data Mean","Post Mean","Mu0"),pch=19,col=c(1,2,4))

```

Sample new data point's value, from the model, which is normal for the data distribution, centered with sigma squared, using the sample of j that we already saw. This is like adding a song to an existing key. This is the distribution of their posterior popularity.


```{r echo=TRUE, fig.height=6, fig.width=7, message=FALSE, warning=FALSE}
## sampling distribution of new observation 
## from a currently existing group

ystar.group1 <- rep(NA,numsamp)
ystar.group2 <- rep(NA,numsamp)
for (i in 1:numsamp){
	ystar.group1[i] <- rnorm(1,mean=mu.samp[i,1],sd=sqrt(truesigsq))
	ystar.group2[i] <- rnorm(1,mean=mu.samp[i,2],sd=sqrt(truesigsq))
}

par(mfrow=c(2,1))
xmin <- min(c(ystar.group1,ystar.group2))
xmax <- max(c(ystar.group1,ystar.group2))
hist(ystar.group1,main="Group 1 New Obs",xlim=c(xmin,xmax))
hist(ystar.group2,main="Group 2 New Obs",xlim=c(xmin,xmax))

```

Alternatively, hierarchical allows us to create a new key. Using shared information across keys, and artists, we compare a new song from the prior distribution across all keys. 
```{r echo=TRUE, fig.height=6, fig.width=7, message=FALSE, warning=FALSE}
## sampling distribution of new observation
## from an entirely new group

ystar.newgroup <- rep(NA,numsamp)
for (i in 1:numsamp){
	mu.newgroup <- rnorm(1,mean=mu0.samp[i],sd=sqrt(tausq.samp[i]))
	ystar.newgroup[i] <- rnorm(1,mean=mu.newgroup,sd=sqrt(truesigsq))
}

par(mfrow=c(3,1))
xmin <- min(c(ystar.group1,ystar.group2,ystar.newgroup))
xmax <- max(c(ystar.group1,ystar.group2,ystar.newgroup))
hist(ystar.group1,main="Group 1 New Obs",xlim=c(xmin,xmax))
hist(ystar.group2,main="Group 2 New Obs",xlim=c(xmin,xmax))
hist(ystar.newgroup,main="New Group New Obs",xlim=c(xmin,xmax))



```

## GIBBS Hierarchical

Beyond three parameters, we need something better than the grid method. MCMC harnesses the ability of optimization algorithms, such as EM, to move around the posterior space and find optimal grid values. The MCMC optimization algorithm specifically builds on each previous iteration. Using MCMC, we establish a path through the posterior instead of randomly sampling through any one of the posteriors at a time. Given our entire path, the current sample is based on the previous sample, and the distribution is based on the immediate previous sample. 

The workhorse of the Monte Carlo is the Gibbs Sampler, which samples from the joint posterior distribution by iteratively sampling from each distribution of each parameter, given each other parameter. Here, we start with some arbitrary values, and from those values, sample from the conditional distribution given the other values of each parameter. Given that current sample, we sample the next iteration, and cycle through all the parameters until we sample theta from the last parameter. 

```{r echo=FALSE, fig.height=6, fig.width=7, message=FALSE, warning=FALSE}

####
### step 6: Hierarchical Gibbs Sampling
####

y <- y_animal_collective
par(mfrow=c(1,1))


##Calculating necessary statistics:
m <- length(y[1,])
n <- rep(NA,m)
means <- rep(NA,m)
for (i in 1:m){
  n[i] <- length(y[,i])
  means[i] <- mean(y[,i])
}
ntot <- sum(n)


numsamp <- 10000
sigsq.samp <- rep(NA,numsamp)
tausq.samp <- rep(NA,numsamp)
mu0.samp <- rep(NA,numsamp)
mu.samp <- matrix(NA,nrow=numsamp,ncol=m)
sigsq <- 1
tausq <- 1
mu0 <- 1 
mu <- rep(NA,m)
for (i in 1:numsamp){
  # sampling mu's
  for (j in 1:m){
    curvar <- 1/(n[j]/sigsq + 1/tausq)
    curmean <- (means[j]*n[j]/sigsq + mu0/tausq)*curvar
    mu[j] <- rnorm(1,mean=curmean,sd=sqrt(curvar))
  }
  # sampling mu0
  mu0 <- rnorm(1,mean=mean(mu),sd=sqrt(tausq/m))
  # sampling tausq
  sumsq.mu <- sum((mu-mu0)^2)
  tausqinv <- rgamma(1,shape=((m-1)/2),rate=(sumsq.mu/2))
  tausq <- 1/tausqinv
  # sampling sigsq
  sumsq.y <- 0
  for (j in 1:m){
    sumsq.y <- sumsq.y + sum((y[,j]-mu[j])^2)
  }
  sigsqinv <- rgamma(1,shape=(ntot/2),rate=(sumsq.y/2))
  sigsq <- 1/sigsqinv
  # storing sampled values
  mu.samp[i,] <- mu
  mu0.samp[i] <- mu0
  tausq.samp[i] <- tausq
  sigsq.samp[i] <- sigsq
  # print(i)
}

```

Markov Chain builds dependence on the previous sample: both a blessing and a curse. While we are moving based on the last move (as in a random walk), we can climb the hill of the posterior distrinution from any starting point. However, we must address the dependence on prior values, first by manually evaluating convergence byvisualizing where the chains come together, and second by reducing the autocorrelation of the paraemter values in each chain. 

In the sample below, we run the GIBBS to sample theta from the i'th iteration, for 10,000 iterations. We establish convergence , seeing whether the black and red lines "come together". Takes about 200 iterations. Define the pre-defined parts as burn in. In other words, throw out a whole 100 values of the chain. The first 100 values are "throw out" as the burn in. For each parameter, we throw out the first 100, and then consider these to be confident samples from the posterior. 

```{r echo=FALSE, fig.height=6, fig.width=7, message=FALSE, warning=FALSE}

### run another chain starting at mu0=20 and tausq=sigsq=10
sigsq.samp2 <- rep(NA,numsamp)
tausq.samp2 <- rep(NA,numsamp)
mu0.samp2 <- rep(NA,numsamp)
mu.samp2 <- matrix(NA,nrow=numsamp,ncol=m)
sigsq <- 50
tausq <- 50
mu0 <- 30
mu <- rep(NA,m)
for (i in 1:numsamp){
  # sampling mu's
  for (j in 1:m){
    curvar <- 1/(n[j]/sigsq + 1/tausq)
    curmean <- (means[j]*n[j]/sigsq + mu0/tausq)*curvar
    mu[j] <- rnorm(1,mean=curmean,sd=sqrt(curvar))
  }
  # sampling mu0
  mu0 <- rnorm(1,mean=mean(mu),sd=sqrt(tausq/m))
  # sampling tausq
  sumsq.mu <- sum((mu-mu0)^2)
  tausqinv <- rgamma(1,shape=((m-1)/2),rate=(sumsq.mu/2))
  tausq <- 1/tausqinv
  # sampling sigsq
  sumsq.y <- 0
  for (j in 1:m){
    sumsq.y <- sumsq.y + sum((y[,j]-mu[j])^2)
  }
  sigsqinv <- rgamma(1,shape=(ntot/2),rate=(sumsq.y/2))
  sigsq <- 1/sigsqinv
  # storing sampled values
  mu.samp2[i,] <- mu
  mu0.samp2[i] <- mu0
  tausq.samp2[i] <- tausq
  sigsq.samp2[i] <- sigsq
  # print(i)
}


```



```{r echo=FALSE, fig.height=6, fig.width=7, message=FALSE, warning=FALSE}

### checking convergence 
par(mfrow=c(2,2))
ymin <- min(mu0.samp,mu0.samp2)
ymax <- max(mu0.samp,mu0.samp2)
plot(1:numsamp,mu0.samp,type="l",main="mu0",ylim=c(ymin,ymax))
lines(1:numsamp,mu0.samp2,col=2)
ymin <- min(tausq.samp,tausq.samp2)
ymax <- max(tausq.samp,tausq.samp2)
plot(1:numsamp,tausq.samp,type="l",main="tausq",ylim=c(ymin,ymax))
lines(1:numsamp,tausq.samp2,col=2)
ymin <- min(sigsq.samp,sigsq.samp2)
ymax <- max(sigsq.samp,sigsq.samp2)
plot(1:numsamp,sigsq.samp,type="l",main="sigsq",ylim=c(ymin,ymax))
lines(1:numsamp,sigsq.samp2,col=2)
ymin <- min(mu.samp[,1],mu.samp2[,1])
ymax <- max(mu.samp[,1],mu.samp2[,1])
plot(1:numsamp,mu.samp[,1],type="l",main="mu 1",ylim=c(ymin,ymax))
lines(1:numsamp,mu.samp2[,1],col=2)

### checking convergence (focussing on first 500 iters)
par(mfrow=c(2,2))
ymin <- min(mu0.samp[1:500],mu0.samp2[1:500])
ymax <- max(mu0.samp[1:500],mu0.samp2[1:500])
plot(1:500,mu0.samp[1:500],type="l",main="mu0",ylim=c(ymin,ymax))
lines(1:500,mu0.samp2[1:500],col=2)
ymin <- min(tausq.samp[1:500],tausq.samp2[1:500])
ymax <- max(tausq.samp[1:500],tausq.samp2[1:500])
plot(1:500,tausq.samp[1:500],type="l",main="tausq",ylim=c(ymin,ymax))
lines(1:500,tausq.samp2[1:500],col=2)
ymin <- min(sigsq.samp[1:500],sigsq.samp2[1:500])
ymax <- max(sigsq.samp[1:500],sigsq.samp2[1:500])
plot(1:500,sigsq.samp[1:500],type="l",main="sigsq",ylim=c(ymin,ymax))
lines(1:500,sigsq.samp2[1:500],col=2)
ymin <- min(mu.samp[1:500,1],mu.samp2[1:500,1])
ymax <- max(mu.samp[1:500,1],mu.samp2[1:500,1])
plot(1:500,mu.samp[1:500,1],type="l",main="mu 1",ylim=c(ymin,ymax))
lines(1:500,mu.samp2[1:500,1],col=2)


### throwing away first 100 as burn-in 

mu.samp <- mu.samp[101:numsamp,]
mu.samp2 <- mu.samp2[101:numsamp,]
mu0.samp <- mu0.samp[101:numsamp]
mu0.samp2 <- mu0.samp2[101:numsamp]
tausq.samp <- tausq.samp[101:numsamp]
tausq.samp2 <- tausq.samp2[101:numsamp]
sigsq.samp <- sigsq.samp[101:numsamp]
sigsq.samp2 <- sigsq.samp2[101:numsamp]

numpostburn <- length(mu0.samp)
numpostburn

### autocorrelation of draws
par(mfrow=c(4,2))
acf(mu0.samp,main="mu0")
acf(mu0.samp2,main="mu0")
acf(sigsq.samp,main="sigsq")
acf(sigsq.samp2,main="sigsq")
acf(tausq.samp,main="tausq")
acf(tausq.samp2,main="tausq")
acf(mu.samp[,1],main="mu 1")
acf(mu.samp2[,1],main="mu 1")


### thinning chains (every 2nd iteration) and checking acf

temp <- 2*c(1:(numpostburn/2))
mu0.samp <- mu0.samp[temp]
mu0.samp2 <- mu0.samp2[temp]
sigsq.samp <- sigsq.samp[temp]
sigsq.samp2 <- sigsq.samp2[temp]
tausq.samp <- tausq.samp[temp]
tausq.samp2 <- tausq.samp2[temp]
mu.samp <- mu.samp[temp,]
mu.samp2 <- mu.samp2[temp,]

par(mfrow=c(4,2))
acf(mu0.samp,main="mu0")
acf(mu0.samp2,main="mu0")
acf(sigsq.samp,main="sigsq")
acf(sigsq.samp2,main="sigsq")
acf(tausq.samp,main="tausq")
acf(tausq.samp2,main="tausq")
acf(mu.samp[,1],main="mu 1")
acf(mu.samp2[,1],main="mu 1")


### combining chains

mu.final <- rbind(mu.samp,mu.samp2)
mu0.final <- c(mu0.samp,mu0.samp2)
tausq.final <- c(tausq.samp,tausq.samp2)
sigsq.final <- c(sigsq.samp,sigsq.samp2)

### find the tau and sig
mean(tausq.final)
mean(sigsq.final)
var(y_animal_collective$B.major)
var(y_animal_collective$C..major)

### posterior distributions of parameters (lines for true values)
par(mfrow=c(1,3))
hist(mu0.final,main="mu0")
mean(means)
abline(v=25,col=2,lwd=2)
hist(sigsq.final,main="sigsq")
mean(sigsq.final)
abline(v=91,col=2,lwd=2)
hist(tausq.final,main="tausq")
mean(tausq.final)
abline(v=62,col=2,lwd=2)
mean(tausq.final)

## these are posterior means for each Key
mu.postmean <- apply(mu.final,2,mean)
mu.postmean
# this is the psoterior overall mean
mu0.postmean <- mean(mu0.final)
mu0.postmean

### Examining Shrinkage
par(mfrow=c(1,1))
plot(1:14,means,main="Shrinkage of Normal Means",pch=19)
abline(h=mu0.postmean,col=4,lwd=2)
points(1:14,mu.postmean,pch=19,col=2)
legend("topright",c("Data Mean","Mu PostMean","Mu0 PostMean"),pch=19,col=c(1,2,4))


## calculate the mean, median, and 95% posterior interval for these

# 1000 samples of each, the 95% posterior intervals:
keys <- names(y_animal_collective)
data_collection <- data.frame(keys)
data_collection$mean <- c(1:14)
data_collection$lower.bound <- c(1:14)
data_collection$upper.bound <- c(1:14)
data_collection$observed <- c(1:14)

sort_try <- sort(mu.final[,1])
row1 <- c(sort_try[248], sort_try[9405], median(mu.samp[,1]), 
          mean(y_animal_collective$A.major))
sort_try <- sort(mu.final[,2])
row2 <- c(sort_try[248], sort_try[9405], median(mu.samp[,2]), 
          mean(y_animal_collective$A.minor))
length(sort_try)
sort_try <- sort(mu.final[,3])
row3 <- c(sort_try[248], sort_try[9405], median(mu.samp[,3]), 
          mean(y_animal_collective$B.major))
sort_try <- sort(mu.final[,4])
row4 <- c(sort_try[248], sort_try[9405], median(mu.samp[,4]), 
          mean(y_animal_collective$C.major))
sort_try <- sort(mu.final[,5])
row5 <- c(sort_try[248], sort_try[9405], median(mu.samp[,5]), 
          mean(y_animal_collective$C..major))
sort_try <- sort(mu.final[,6])
row6 <- c(sort_try[248], sort_try[9405], median(mu.samp[,6]), 
          mean(y_animal_collective$C.minor))
sort_try <- sort(mu.final[,7])
row7 <- c(sort_try[248], sort_try[9405], median(mu.samp[,7]), 
          mean(y_animal_collective$D.major))
sort_try <- sort(mu.final[,8])
row8 <- c(sort_try[248], sort_try[9405], median(mu.samp[,8]), 
          mean(y_animal_collective$D..major))
sort_try <- sort(mu.final[,9])
row9 <- c(sort_try[248], sort_try[9405], median(mu.samp[,9]), 
          mean(y_animal_collective$E.major))
sort_try <- sort(mu.final[,10])
row10 <- c(sort_try[248], sort_try[9405], median(mu.samp[,10]), 
          mean(y_animal_collective$E.minor))
sort_try <- sort(mu.final[,11])
row11 <- c(sort_try[248], sort_try[9405], median(mu.samp[,11]), 
          mean(y_animal_collective$F.major))
sort_try <- sort(mu.final[,12])
row12 <- c(sort_try[248], sort_try[9405], median(mu.samp[,12]), 
          mean(y_animal_collective$F..major))
sort_try <- sort(mu.final[,13])
row13 <- c(sort_try[248], sort_try[9405], median(mu.samp[,13]), 
          mean(y_animal_collective$G.major))
sort_try <- sort(mu.final[,14])
row14 <- c(sort_try[248], sort_try[9405], median(mu.samp[,14]), 
          mean(y_animal_collective$G..major))
data_collection2<- rbind(row1, row2, row3, row4, row5, row6,
                         row7, row8, row9, row10,
                         row11, row12, row13, row14)
data_collection2 <- as.data.frame(data_collection2)
names(data_collection2)[1] <- "lower bound"
names(data_collection2)[2] <- "upper bound"
names(data_collection2)[3] <- "median"
names(data_collection2)[4] <- "observed"
row.names(data_collection2) <- keys
data_collection2 <- cbind(data_collection2, mu.postmean)

kbl(data_collection2) %>%
  kable_paper("hover", full_width = F) %>%
  row_spec(1, bold = T) %>%
  row_spec(2, bold = T) %>%
  add_header_above(c("Posterior Inference for Thetas", "", "", "", "", ""))



```



## Posterior Intervals


```{r eval=FALSE, fig.height=6, fig.width=7, message=FALSE, warning=FALSE, include=FALSE}

############
### Step 7: Checking the most popular song
############

### this has to be a sample: sample 5 songs from each
load("/cloud/project/data/output_bayes (2).Rdata")
merge <- new_data
merge <- merge %>%
  select(track_name, artist_name, key_mode, popularity_index, album_name)
merge <- merge %>%
  arrange(track_name, popularity_index) %>%
  filter(duplicated(track_name) == FALSE)
merge <- merge[- grep("-", merge$track_name),]
merge <- merge %>%
  arrange(track_name, popularity_index) %>%
  filter(duplicated(track_name) == FALSE)
merge <- merge[- grep("live", merge$track_name),]
merge <- merge[- grep("Live", merge$track_name),]

### choose a sampling procedure for this
merge$pop <- as.numeric(merge$popularity_index)
animal <- merge %>%
  filter(artist_name == "Animal Collective") %>%
  select(key_mode, pop, album_name, track_name) %>%
  group_by(key_mode) %>%
  filter(pop > 40)
animal_A <-merge %>%
  filter(artist_name == "Animal Collective") %>%
  filter(key_mode == "A major")%>%
  select(track_name, album_name, pop)  %>%
  arrange(desc(pop))
animal_D <-merge %>%
  filter(artist_name == "Animal Collective") %>%
  filter(key_mode == "D major")%>%
  select(track_name, album_name, pop)  %>%
  arrange(desc(pop))
animal_E <-merge %>%
  filter(artist_name == "Animal Collective") %>%
  filter(key_mode == "E minor")%>%
  select(track_name, album_name, pop)  %>%
  arrange(desc(pop))
kbl(animal_A, col.names = NULL) %>%
  kable_paper("hover", full_width = F) %>%
  add_header_above(c("Song", "Album", "Popularity")) %>%
  add_header_above(c("Key of A Major", "", ""))
kbl(animal_D, col.names = NULL) %>%
  kable_paper("hover", full_width = F) %>%
  add_header_above(c("Song", "Album", "Popularity")) %>%
  add_header_above(c("Key of D Major", "", ""))
kbl(animal_E, col.names = NULL) %>%
  kable_paper("hover", full_width = F) %>%
  add_header_above(c("Song", "Album", "Popularity")) %>%
  add_header_above(c("Key of E Minor", "", ""))



```

## Citations
